{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p style=\"text-align:center;\"><font size=6>FREESPACE SEGMENTATION WITH PYTORCH</font></p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project's purpose is finding the driveable area (or otherwise freespace) in the high way road. Which is quite a large subject when thinking of the high way's elements.Like lines, debris, barriers, etc. Although this is not the general subject, in the below sections dashed line, solid line, and traffic sign models added to the project.\n",
    "<br>\n",
    "Codes left in the files but important ones are explained and exampled, also files can be visible in the repository.\n",
    "<br>\n",
    "This project developed on Windows 10 with Anaconda (python3) and CUDA available graphic card.\n",
    "<br>\n",
    "To see the project process, steps are explained below:\n",
    "<div>\n",
    "    <ul>\n",
    "        <li>JSON to Mask: \"Freespace\" (as class title) searched in the JSON files. After that, pattern drawn to the empty mask as 0 1 type</li>\n",
    "        <li>Preprocess: Masks and images converted to the tensors. One difference between image and mask tensor is the one hot encode for the mask.</li>\n",
    "        <li>Model: For the project, the UNet model was selected(according to preference).</li>\n",
    "        <li>Train: Dataset separated 3 parts to train, validation, and test. The predictions of the model are taken in this section. </li>\n",
    "        <li>Dashed Line, Solid Line, and Traffic Sign Detection</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5> JSON to Mask**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON files contain lots of content about the annotated image like class titles, points of objects, height and width of the image, etc. Firstly all of the JSON files must be loaded and then should be open in order. For the pattern of mask,freespace needed to found. Once freespace found, a pattern must be drawn an empty mask as 0 1 type.1 is represent the freespace, and 0 present the other pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries need to be imported for this part:\n",
    "<ul>\n",
    "    <li>import numpy as np</li>\n",
    "    <li>import cv2</li>\n",
    "    <li>import json</li>\n",
    "    <li>import os</li>\n",
    "    <li>from os import listdir</li>\n",
    "    <li>from os.path import isfile,join</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the JSON files, the path of their folder and images folder must be defined.JSON files can be added to the list with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=[f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open JSON files in order and found freespace, for loop used. Also, files are loaded for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_names)):\n",
    "    fileopen=open(mypath+\"\\\\\"+file_names[i]) \n",
    "\n",
    "    json_dict = json.load(fileopen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask of the picture must be the same size as the picture, so width and height were taken from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_image=json_dict['size']['height']\n",
    "w_image=json_dict['size']['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mask started by creating an empty mask with the help of the NumPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.zeros((h_image,w_image),dtype= np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the outline of the freespace is created. Points of the freespace are needed to create the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objs=json_dict[\"objects\"]  \n",
    "for obj in json_objs:\n",
    "        class_name=obj['classTitle'] #freespace required\n",
    "        if class_name == \"Freespace\" :\n",
    "            p=obj['points']          \n",
    "            freespace_points=p['exterior'] #freespace's points x,y type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is set up to create the pattern.0 for the other pixels and 1 for the freespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.fillPoly(mask,np.array([freespace_points]),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving .png file, mask may appear full black, but when written as an array, 0s and 1s can be seen. To check the result, a mask can be drawn on its image with following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask  = cv2.cv2.imread(mask_path, 0).astype(np.uint8)\n",
    "image = cv2.cv2.imread(image_path).astype(np.uint8)\n",
    "\n",
    "mask_ind   = mask == 1\n",
    "cpy_image  = image.copy()\n",
    "image[mask==1,:] = (0, 165, 255)\n",
    "opac_image = (image/2 + cpy_image/2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code's progress is to read the mask and image as uint8 type after that find the numbers in mask equal to 1 which is freespace representation after that color them with color code (0,165,255) on the image.\n",
    "<br>\n",
    "Dividing image and cpy_image to 2, gives us a colored part on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**<font size=5> Preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and masks must be tensor to train the model. But there is one difference between converting the mask and image to tensor. It's one hot encoding. Masks are drawn 0 and 1 type before. The function of one-hot encoding is label 1s as freespaces for the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the tensor function, both of them (image and mask) are resized for the standard input. Images are RGB (3 channel) and masks are GrayScale (2 channel) type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that: CUDA usage makes these processes faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the image to the output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.resize(image, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the image_list to the NumPy array and after that convert the NumPy list to the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(image_list, dtype=np.float32)\n",
    "torch_image = torch.from_numpy(image_array).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the CUDA usage following code needed to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if cuda:\n",
    "        torch_image = torch_image.cuda()\n",
    "    return torch_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image tensors are in [batch_size, output_shape[0], output_shape[1], 3] shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are applied to the mask as well. After the resize mask following code applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = one_hot_encoder(mask, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(data, n_class):\n",
    "    encoded_data = np.zeros((*data.shape, n_class), dtype=np.int)\n",
    "    encoded_labels = [[0,1], [1,0]]\n",
    "    for lbl in range(n_class):\n",
    "        encoded_label = encoded_labels[lbl]\n",
    "        numerical_class_inds = data[:,:] == lbl\n",
    "        encoded_data[numerical_class_inds] = encoded_label\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written earlier freespaces(1s as according to preference) are labeled in order. Function returns encoded data.\n",
    "<br>\n",
    "After masks converted to tensor,tensor shape is [batch_size, output_shape[0], output_shape[1], 2]. 2 for the grayscale image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5> Model**\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"unet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">image source[5](from unet paper)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet contains two parts. One part is downsampling, then the resolution of the picture increases with upsampling. Max-pooling and ReLu were used in the downsampling part of UNet.And the upsampling part bilinear interpolation used in the upsample function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function:**Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly known activation functions: Softmax, Relu, Leaky ReLu, and Sigmoid function.\n",
    "<br>\n",
    "ReLu activation function used in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLu:**ReLu is a non-linear activation function that is used in multi-layer neural networks or deep neural networks.\n",
    "<br>\n",
    "This function can be represented as: f(x) =max(0,x)\n",
    "<br>\n",
    "According to equation 1, the output of ReLu is the maximum value between zero and the input value. The output is equal to zero when the input value is negative and the input value when the input is positive.[1]\n",
    "<br>\n",
    "**graph source[2]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"relu.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel:** Applied filter mask.\n",
    "<br>\n",
    "**Stride:** The number of steps.\n",
    "<br>\n",
    "**Maxpooling**:The largest pixel in that area is selected according to the kernel size.Complexity is reduced by keeping height and width information constant.Below image kernel_size=2 with stride=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"maxpooling.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the kernel size does not divide the picture exactly, padding can be applied.Padding adds extra pixels around the image and prevents displacement.Also stride can be changeable according to the project.In the following image padding=1 applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"padding.jpg\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upsampling**:Upsample puts new values between the original pixels to increase the image size. While adding pixels, upsampling uses functions. This model contains the bilinear interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bilinear Interpolation**:Bilinear interpolation is a technique for calculating values of a grid location-based on nearby grid cells. The key difference is that it uses the FOUR closest cell centers.\n",
    "Using the four nearest neighboring cells, bilinear interpolation assigns the output cell value by taking the weighted average. It applies weights based on the distance of the four nearest cell centers smoothing the output raster grid.[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.cat() function:**Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conv2d function:**Applies a 2D convolution over an input signal composed of several input planes.[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fully considered, codes added below(for full view model.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.down1 = m_conv(3, 64)\n",
    "        self.down2 = m_conv(64, 128)\n",
    "        self.down3 = m_conv(128, 256)\n",
    "        self.down4 = m_conv(256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.maxpooling = nn.MaxPool2d(2)\n",
    "self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.up3 = m_conv(256 + 512, 256)\n",
    "self.up2 = m_conv(128 + 256, 128)\n",
    "self.up1 = m_conv(128 + 64, 64)\n",
    "\n",
    "self.last = nn.Conv2d(64, n_classes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        conv1 = self.down1(x)\n",
    "        x = self.maxpooling(conv1)\n",
    "\n",
    "        conv2 = self.down2(x)\n",
    "        x = self.maxpooling(conv2)\n",
    "\n",
    "        conv3 = self.down3(x)\n",
    "        x = self.maxpooling(conv3)\n",
    "\n",
    "        x = self.down4(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.up1(x)\n",
    "\n",
    "        out_layer = self.last(x)\n",
    "        return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=\"5\"><b>Train </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple different metrics were evaluated while training the model. The model with the best result (with the least loss) was selected as the main model and the results from the evaluations are added below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function given in the script was giving an error, so MSE loss was used instead.Both(script and other selected) optimizers have applied the model with the same epoch number for the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, validation, and test data are not separated in Dataset.\n",
    "First of all, the data is separated by percentage.In this part, normalization can be applied to the data according to preference.\n",
    "<br>\n",
    "The values valid in the project should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.3\n",
    "test_size  = 0.1\n",
    "batch_size = 4\n",
    "epochs = 20 #this will change during the project for the evaluation\n",
    "cuda = True\n",
    "input_shape = (224, 224)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of mask and images are defined and files in folders are uploaded. Since all definitions are made, dataset separation can be made with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLICE TEST DATASET FROM THE WHOLE DATASET\n",
    "test_input_path_list = image_path_list[:test_ind]\n",
    "test_label_path_list = mask_path_list[:test_ind]\n",
    "\n",
    "\n",
    "\n",
    "# SLICE VALID DATASET FROM THE WHOLE DATASET\n",
    "valid_input_path_list = image_path_list[test_ind:valid_ind]\n",
    "valid_label_path_list = mask_path_list[test_ind:valid_ind]\n",
    "\n",
    "\n",
    "\n",
    "# SLICE TRAIN DATASET FROM THE WHOLE DATASET\n",
    "train_input_path_list = image_path_list[valid_ind:]\n",
    "train_label_path_list = mask_path_list[valid_ind:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model call and optimizer, loss definition was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL MODEL\n",
    "model = FoInternNet(n_classes=2)\n",
    "\n",
    "# DEFINE LOSS FUNCTION AND OPTIMIZER\n",
    "criterion =  nn.MSELoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA usage checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF CUDA IS USED, IMPORT THE MODEL INTO CUDA\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire train part can be seen in the train.py file.\n",
    "<br>\n",
    "Test data allocated to get the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in test_input_path_list:\n",
    "    shutil.copy(image_test,'..\\\\folder')\n",
    "for msk in test_label_path_list:\n",
    "    shutil.copy(mask_test,'..\\\\folder') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='..\\\\model_name.pt'\n",
    "torch.save(model,PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used to draw the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_training= [float(i)/max(traning_loss_list) for i in traning_loss_list]\n",
    "normalized_validation=[float(j)/max(validation_loss_list) for j in validation_loss_list]\n",
    "plt.plot(normalized_training,label='training loss list')\n",
    "plt.plot(normalized_validation,label='validation loss list')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the optimizer, model trained with 5 epoch Adam optimizer and SGD optimizer. It was a quick look at the loss change.\n",
    "According to the results, it was more correct to use Adam optimizer for the UNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likely MSE Loss function + Adam optimizer looked the best option. After the functions are decided, different epoch numbers are tried on the model.\n",
    "<br>\n",
    "At the left, the model trained with 10 epoch(every epoch takes about 716 images), and at the right epoch number is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comprasion.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why validation error is less than training error in the Epoch 20 graph maybe that training data is easier than validation data.The desired graphic is that the validation loss is slightly more than the training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs of the model(unnormalized) directly converted from tensor to image with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(output,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was then uploaded and predicted on the test data. While evaluating, eval() was used to disable layers such as Batch Normalization and Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data given to the model must be tensor, so the tensorize_image function used in the preprocessing stage should be used. Afterward, prediction can be taken from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(tesorized_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predictions according to the number of epoch without making any changes to the data are given as 10 epoch 20 epoch and the ground truth, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roadcar.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 10 epoch, the model mostly made the wrong prediction around the car. In the 20 epoch, the model mostly caught the right prediction around the car but still, some wrong predictions were there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a case, increasing the number of epochs seems like a good choice. But if the loss of the validation increases, the model starts to memorize it. When the model memorizes the pictures, it makes false predictions in the test data (has never seen). Dropout or augmentation can be used to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to examine situations that are not usually encountered on the road, such as viaducts and tunnels that may confuse the prediction of the model, to reach a correct working model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"viaduct.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model behaved consistently in changing lights and did not perceive the viaduct as freespace, but made incorrect predictions, especially around the vehicles and freespace's road border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the examination of the tunnels where the light is diminished, which will make the model's prediction much more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tunnel.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model also perceived the walls of the tunnel as freespaces due to the light changes. As the training time increased, the freespaces on the wall decreased. However, it is not the desired prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to look at the counters to see how the model predicts when the light is normal but the elements on the road are excessive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"counter.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the model on the counters seems wrong due to the color of the concrete barriers.\n",
    "Although the freespace selected area decreases as the number of epochs increases, the model's prediction is far from expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long vehicles such as trucks and trailers can present a situation similar to tunnel walls, causing confusion in the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"truck.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perceived the truck as freespace but that was a wrong prediction. Even increasing the number of epoch couldn't solve the wrong prediction. Augmentation can handle this and similar situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset of the project is not enough for the model to make accurate predictions in light changes or different situations. In such a case, augmentation can be applied to the dataset.\n",
    "<br>\n",
    "**Data Augmentation:** Data augmentation is a technique for modifying the data used in the project. It has options such as adding noise, changing images angle, rotating or changing images contrast. Mirroring, one of the most basic technique, was applied to the dataset for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aug1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation does not have to be applied to the whole data, can be applied only to selected ones, but it must also be applied to the mask of that image. Increasing the number of pictures for training, normally increases the training time. This technique also prevents the model from being overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comparison of the losses of the models in the same number of epochs. (Epoch 20,adam optimizer and MSE Loss function)\n",
    "<br>\n",
    "On the left-side graph before augmentation and the right-side graph after augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comprrasion.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model's validation loss has peak before the augmentation and augmentation prevent that in the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the predictions of the model are expected to be closer to the ground truth than the previous ones. Comparing with other situations and seeing what kind of changes will be beneficial in terms of how to follow the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top picture is the ground truth, and the model's prediction left to right is epoch 10 epoch 20 and after augmentation epoch 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roadcar2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation did not provide the full benefit, rather it caused distortions in close range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"viaduct2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has done quite well around the vehicle at close range and still has trouble determining the boundaries of the road. In general, it can be seen that the prediction of the model is starting to become definite. The prediction of the model in the viaduct compared to other training has improved and made a prediction close to the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tunnel2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tunnels, which is one of the situations where the model has difficulty in making a decision, it is seen that the freespace selected on the walls of the tunnel after augmentation decreases, and the freespace selected on the road increases. It is seen that augmentation improves the model's predictions in tunnels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"counter2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still makes similar predictions about the concrete barriers in the counter. The benefit of augmentation is that it increases the freespace parts on the road. Applying more complex augmentation can reduce freespace prediction in concrete barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"truck2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After augmentation, the model does not choose the truck as freespace and predicted a result very close to ground truht.The model's progress looks good in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparisons, it seems that augmentation increases the accuracy of the model's predictions, bringing it closer to the ground truth. As the process applied during augmentation becomes more complex, the accuracy rate will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=\"5\"><b>Line Detection</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>Solid Line</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly started with Solid Line Detection. The whole process is the same as in the freespace section. Only the code in the JSON to mask section has been changed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=obj['classTitle'] #solid line required\n",
    "        if class_name == \"Solid Line\" :\n",
    "            p=obj['points']          \n",
    "            solidline_points=p['exterior'] #solid line's points x,y type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once points are found line or lines(It may differ according to JSON) must be drawn to the empty mask with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=cv2.cv2.polylines(mask,np.array([solidline_points]),False,1,thickness=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables in order to:** mask(empty), points of solid lines, open or closed lines preference, the color of lines, and width of lines.The choice of open or closed means whether a line will be drawn between the start and endpoints of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After masks created ground truth can be checkable with the mask on image function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"groundline.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison was made on graphs to measure the performance of the model. Started with 10 epochs on the left side. In the middle 10 epoch with dropout and finally 16 epoch with dropout at right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout:** Used to prevent overfitting in the model. The dropout technique disables some nodes in the layer and leaves active the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graph.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the left graph, the validation loss is quite high. After this result, dropout seems like a good option to reduce validation loss. Dropout and increase in the epoch number decreased the validation loss likely the other one.\n",
    "Dropout implemented the model with nn.Dropout(p=0.5) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After increasing the number of epochs to 15, the model's predictions were compared with ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above picture contains model prediction with 15 epoch and dropout, model prediction on the image, and the ground truth, respectively.\n",
    "<br>\n",
    "The model was also successful in the line with less confusion, but there are deficiencies in the prediction on the side where there are many vehicles and the line does not appear from time to time.\n",
    "<br>\n",
    "On the other, the model's prediction very successfully on one line in close range, but made a mistake at the beginning of the other line.\n",
    "<br>\n",
    "From these comparisons, it is clear that the model still needs training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model made predictions in open areas with good light, it was not successful in the presence of counter, tunnel, and trucks. The following picture shows model prediction, prediction on the image, and ground truth, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bad.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be understood from here, it is necessary to increase the training time of the model and to increase its training in different situations by applying augmentation to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=\"3\"><b>Dashed Line</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, all operation is the same as the others. The part to be changed is the class name searched in JSON in the JSON to mask section. Code and ground truth of image in the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=obj['classTitle'] #dashed line required\n",
    "        if class_name == \"Dashed Line\" :\n",
    "            p=obj['points']          \n",
    "            dashedline_points=p['exterior'] #dashed line's points x,y type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the same code as on the solid line part for the draw mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=cv2.cv2.polylines(mask,np.array([dashedline_points]),False,1,thickness=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dashed.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the performance of the model in epoch 10 and epoch 20 without dropout. In the second row, the performance of the model in epoch 10 and epoch 15 after applying p = 0.5 dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dashedgraph.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch could not be increased to 20 because of GPU out of memory error while implementing dropout. As can see the graphs, implementing dropout reduces the sudden increases at the validation loss.\n",
    "<br>\n",
    "When applying the prediction of the model on the pictures, the model trained with 20 epoch was used, as it was seen that 15 epoch did not produce sufficient results. Training the model with 20 or more dropouts may give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the model trained with 20 epochs are below. Left to the right contains a model prediction, model prediction on the image, and ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dashed20.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in freespace and solid line, it is seen that the model has difficulties in predictions in situations that are not often encountered here. Applying augmentation as in freespace segmentation will increase the accuracy of the model's predictions on the solid lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=\"5\"><b>Traffic Sign Detection</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection is generally understood and the steps proceed similarly. Traffic Signs from the JSON file were found and drawn to the empty mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in json_objs:\n",
    "        class_name=obj['classTitle'] #traffic sign required\n",
    "        if class_name == \"Traffic Sign\" :\n",
    "            p=obj['points']          \n",
    "            traffic_sign_points=p['exterior'] #traffic sign's points x,y type\n",
    "            sign_x1=traffic_sign_points[0][0]\n",
    "            sign_x2=traffic_sign_points[0][1]\n",
    "            sign_y1=traffic_sign_points[1][0]\n",
    "            sign_y2=traffic_sign_points[1][1]\n",
    "            x=(sign_x1,sign_x2)\n",
    "            y=(sign_y1,sign_y2)\n",
    "            mask=cv2.cv2.rectangle(mask,x,y,1,thickness=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim here is to find the corner points of the rectangle used to annotate the traffic sign. Since the function does not accept points collectively, it was necessary to separate the points. Writing x1 as x2 does not indicate coordinates, it is used as point 1 and 2 of x and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks are drawn to the images for the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ground.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of the model trained with 20 epochs before the augmentation is on the left below. Even losses look low, the model's prediction was very bad. Augmentation was applied due to the lack of traffic signs in the data. After augmentation losses at the right below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"trafficsign.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mirroring, which is one of the augmentation techniques, causes high validation and train loss, a different technique has been tried. The number of pictures in the data has been increased by increasing and decreasing the brightness in the pictures with the help of the PIL library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sign.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for increase brigthness\n",
    "enchancer=ImageEnhance.Brightness(image)\n",
    "factor=1.5\n",
    "image_output=enchancer.enhance(factor)\n",
    "\n",
    "#for descrease brigthness\n",
    "enchancer=ImageEnhance.Brightness(image)\n",
    "factor=0.5\n",
    "image_output=enchancer.enhance(factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was applied to a very small portion of the dataset. It was not applied to masks as the traffic signs were not relocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the signs in other sections are few, the model has also been tested on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"expsign.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that the model is insufficient to learn the signs, interpretation can still be made.\n",
    "<br>\n",
    "It is understood that the model cannot learn the white-based signs from its failure to detect the close range. The model was able to detect the blue signs, although the model could not fully draw its borders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different examples can be used to see better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"expp.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has begun to learn blue signs due to their multiplicity in the data, but despite this, their prediction is quite dispersed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=\"5\"><b>Final Result</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for freespace, solid line, dashed line and traffic sign were trained and drawn on the pictures collectively after their predictions was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mask  = cv2.cv2.imread(f_path, 0).astype(np.uint8)\n",
    "f_mask=cv2.cv2.resize(f_mask,(1920,1208))\n",
    "\n",
    "s_mask  = cv2.cv2.imread(s_path, 0).astype(np.uint8)\n",
    "s_mask=cv2.cv2.resize(s_mask,(1920,1208))\n",
    "\n",
    "d_mask  = cv2.cv2.imread(d_path, 0).astype(np.uint8)\n",
    "d_mask=cv2.cv2.resize(d_mask,(1920,1208))\n",
    "\n",
    "si_mask  = cv2.cv2.imread(si_path, 0).astype(np.uint8)\n",
    "si_mask=cv2.cv2.resize(si_mask,(1920,1208))\n",
    "\n",
    "ground = cv2.cv2.imread(ground_path).astype(np.uint8)\n",
    "\n",
    "\n",
    "    \n",
    "cpy_image  = ground.copy()\n",
    "ground[f_mask==1,:] = (0, 165, 255) #freespace\n",
    "ground1 = (ground/2 + cpy_image/2).astype(np.uint8)\n",
    "\n",
    "\n",
    "cpy_image  = ground1.copy()\n",
    "ground1[s_mask==1,:] = (165, 255, 0) #solid line\n",
    "ground2 = (ground1/2 + cpy_image/2).astype(np.uint8)\n",
    "\n",
    "cpy_image  = ground2.copy()\n",
    "ground2[d_mask==1,:] = (165, 0, 255) #dashed line\n",
    "ground3 = (ground2/2 + cpy_image/2).astype(np.uint8)\n",
    "\n",
    "cpy_image  = ground3.copy()\n",
    "ground3[si_mask==1,:] = (0, 0, 255) #traffic sign\n",
    "ground4 = (ground3/2 + cpy_image/2).astype(np.uint8)\n",
    "\n",
    "\n",
    "cv2.imwrite(ground_out_path, ground4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"result1.jpg\">\n",
    "<br>\n",
    "<img src=\"result2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Reference:https://github.com/cakirogluozan/fo-intern-project\n",
    "UNet Model:https://github.com/usuyama/pytorch-unet/blob/master/pytorch_unet.py only function names changed,dropout implemented after second Conv layer in Sequential.Also optimizer taked from here.\n",
    "\n",
    "[1]https://deepai.org/machine-learning-glossary-and-terms/relu\n",
    "[2]https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "[3]https://gisgeography.com/bilinear-interpolation-resampling/\n",
    "[4]https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253\n",
    "[5]https://arxiv.org/abs/1505.04597 UNet Paper\n",
    "[6]https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "[7]https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
